---
permalink: /
title: "Research Spotlight"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I am a Mathematician, Postdoctoral developer @ [Concordia University, School of Engineering and Computer Science](https://www.concordia.ca/ginacody.html). I completed my last postdoc in computer social media. I built dependence structure between labor occupation based on job counts data,  using copulae. I also developed a theoretical model based on hawkes intensity point processes allows fitting information diffusion based on observed counts of events. The main tools we used were functional analysis and distribution theory. I completed my [PhD](https://www.maths.usyd.edu.au/ut/people?who=LJ_Dong) in early 2018 on the topic of stochastic Navier-Stokes equations on the rotating spheres with stable LÃ©vy noise, under supervision of Prof. Beniamin Goldys, the univeristy of Sydney. I have been teaching in universities on various topics for roughly 10 years. Prior to my PhD, I was a researcher in quantitative finance and stochastic processes.

Currently, two main themes of my research are:

**Compiler for a domain specific programming language**

* Extend the [INSEL](https://www.insel.eu/en/) compiler functionality to sort the model of an undirected graph, that is, the order the nodes and edges represented by blocks
identified by their block numbers;

* Undirected Graph Parser

* Loop detections (with lots of sparsity)

* Automate system of equations

* Generic numerical algorithms for  solving  nonlinear system of equations

**Theorectical and Interpretable Machine Learning**

Motivated from the celebrated work of Prof. Cynthia Rudin, these days I have been studying a rule-based approach to Machine Learning with optimal decision tree.
Decision tree can be viewed as an optimization problem with the objective of
maximize accuracy subjects to a constraint on the size of the decision tree (DT).
The most popular algorithm is DT4.5 date back to the 90s. The algorithm doesn't work very well which motivated some modernization of DT algorithms

One approach is through optimal decision list. The goal is to globally optimize all possible decision lists.
That is, to minimize the loss subject to the model being sparse. Decision list is a one-sided DT and so it is a series of if-then rules.
They are exponentially easier to create than the full-blown decision trees and both DT and decision list to create them optimally is computationally hard,
i.e. NP hard with no polynomial time to approximation. Hence they are very hard optimization problem.
With modern computational power, we can solve optimal decision list with reasonable large dataset in a reasonable amount of time.

More recently, I have been looking at a generalized Sparse decision tree approach. The key difference between decision trees and decision lists is that the former may be viewed as unordered  rule sets, where each leaf of the tree corresponds to a single rule with a condition part consisting of the conjunction of all edge labels on the path from the root to this leaf.

**Topics of interests**: 

* Graph theory and algorithms

* C++ (Mostly modern, post-modern)

* Data structures and algorithms

* Machine/Deep Learning intepretability


<!---

**Past Interests**

* Mathematical Analysis of Artificial Intelligence and Theoretical Computer Science

* Theoretical or Statistical Machine Learning

* Stochastic PDEs, Financial Mathematics

* Quantum Computing, Quantum game theory, Information Geometry, Quantum Machine Learning

* Point processes and applications to Social Media, Finance, Insurance, Quantum Physics

--->

